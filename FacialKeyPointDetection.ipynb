{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FacialKeyPointDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBoludj29Q0QIQ3tDW201U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivgandhi97/FacialKeyPointDetection/blob/main/FacialKeyPointDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvnkONh4Cy-h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the required libraries\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import torch\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "nPxjNZhYC3dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LR = 0.001\n",
        "EPOCHS = 30\n",
        "ROOT_PATH = '/content/drive/MyDrive/Facial-Keypoint-Detection-master/data'"
      ],
      "metadata": {
        "id": "RCTRWfoNC5Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_pts_frame = pd.read_csv('/content/drive/MyDrive/Facial-Keypoint-Detection-master/data/training_frames_keypoints.csv')\n",
        "\n",
        "n = 0\n",
        "image_name = key_pts_frame.iloc[n, 0]\n",
        "key_pts = key_pts_frame.iloc[n, 1:].to_numpy()\n",
        "key_pts = key_pts.astype('float').reshape(-1, 2)\n",
        "\n",
        "print('Image name: ', image_name)\n",
        "print('Landmarks shape: ', key_pts.shape)\n",
        "print('First 4 key pts: {}'.format(key_pts[:4]))"
      ],
      "metadata": {
        "id": "2dwTI2UwC9d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceKeypointDataset(Dataset):\n",
        "    def __init__(self, samples, path, transform = None):\n",
        "        self.data = samples\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = cv2.imread(f\"{self.path}/{self.data.iloc[index][0]}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        keypoints = self.data.iloc[index][1:]\n",
        "        keypoints = np.array(keypoints, dtype='float32')\n",
        "        # reshape the keypoints\n",
        "        keypoints = keypoints.reshape(-1, 2)\n",
        "        \n",
        "        sample = {'image': image, 'keypoints': keypoints}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "4EMp1y3UFPxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "    \n",
        "class Normalize(object):\n",
        "    \"\"\"Normalize the color range to [0,1] and convert a color image to grayscale if needed\"\"\"        \n",
        "    def __init__(self, color = False):\n",
        "        self.color = color\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "        \n",
        "        image_copy = np.copy(image)\n",
        "        key_pts_copy = np.copy(key_pts)\n",
        "\n",
        "        # convert image to grayscale\n",
        "        if not self.color:\n",
        "            image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        \n",
        "        # scale color range from [0, 255] to [0, 1]\n",
        "        image_copy=  image_copy/255.0\n",
        "            \n",
        "        \n",
        "        # scale keypoints to be centered around 0 with a range of [-2, 2]\n",
        "        key_pts_copy = (key_pts_copy - image.shape[0]/2)/(image.shape[0]/4)\n",
        "\n",
        "\n",
        "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
        "\n",
        "\n",
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "            \n",
        "        img = cv2.resize(image, (new_w, new_h))\n",
        "                 \n",
        "        # scale the pts, too\n",
        "        key_pts = key_pts * [new_w / w, new_h / h]\n",
        "        \n",
        "        return {'image': img, 'keypoints': key_pts}\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        top = np.random.randint(0, h - new_h)\n",
        "        left = np.random.randint(0, w - new_w)\n",
        "\n",
        "        image = image[top: top + new_h,\n",
        "                      left: left + new_w]\n",
        "\n",
        "        key_pts = key_pts - [left, top]\n",
        "\n",
        "        return {'image': image, 'keypoints': key_pts}\n",
        "    \n",
        "class FaceCrop(object):\n",
        "    \"\"\" Crop out face using the keypoints as reference\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"       \n",
        "        \n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "\n",
        "        image_copy = np.copy(image)\n",
        "        \n",
        "        h, w = image.shape[:2]\n",
        "        \n",
        "        x_max = 0\n",
        "        x_min = 10000\n",
        "        y_max = 0\n",
        "        y_min = 10000\n",
        "        \n",
        "        # Find the coordinates to keypoints at the far left, far right, top and bottom\n",
        "        # Also check that no keypoints are outside the image\n",
        "        for coord in key_pts:\n",
        "            if coord[0] > x_max:\n",
        "                if coord[0] >= w:\n",
        "                    x_max = w\n",
        "                else:\n",
        "                    x_max = coord[0]\n",
        "            if coord[0] < x_min:\n",
        "                if coord[0] < 0:\n",
        "                    x_min = 0\n",
        "                else:\n",
        "                    x_min = coord[0]\n",
        "            if coord[1] > y_max:\n",
        "                if coord[1] >= h:\n",
        "                    y_max = h\n",
        "                else:\n",
        "                    y_max = coord[1]\n",
        "            if coord[1] < y_min:\n",
        "                if coord[1] < 0:\n",
        "                    y_min = 0\n",
        "                else:\n",
        "                    y_min = coord[1]\n",
        "        \n",
        "        # Set the the left corner keypoint as out crop cooridnate\n",
        "        x = int(x_min)\n",
        "        y = int(y_min)\n",
        "        \n",
        "        # Get height and width of keypoint area\n",
        "        new_h = int(y_max - y_min)\n",
        "        new_w = int(x_max - x_min)\n",
        "        \n",
        "        #Set the smallest side equal to the largest since we want a square\n",
        "        if new_h > new_w:\n",
        "            new_w = new_h\n",
        "        else:\n",
        "            new_h = new_w       \n",
        "        \n",
        "        randsize1 = [2, 70]\n",
        "        randsize2 = [2, 30]\n",
        "        randsize3 = [1, 10]\n",
        "\n",
        "        # Check that padding dosent go outside the frame\n",
        "        padding_x_1 = 0\n",
        "        padding_x_2 = 0\n",
        "        padding_y_1 = 0\n",
        "        padding_y_2 = 0\n",
        "        \n",
        "        padding_size_x_1 = random.randint(randsize1[0],randsize1[1])\n",
        "        padding_size_x_2 = random.randint(randsize1[0],randsize1[1])\n",
        "        padding_size_y_1 = random.randint(randsize1[0],randsize1[1])\n",
        "        padding_size_y_2 = random.randint(randsize1[0],randsize1[1])\n",
        "        if(y - padding_size_y_1 > 0 and x - padding_size_x_1 > 0\n",
        "           and x + new_w + padding_size_x_2 < w and y + new_h + padding_size_y_2 < h):\n",
        "            padding_x_1 = padding_size_x_1\n",
        "            padding_x_2 = padding_size_x_2\n",
        "            padding_y_1 = padding_size_y_1\n",
        "            padding_y_2 = padding_size_y_2\n",
        "        else:\n",
        "            padding_size_x_1 = random.randint(randsize2[0],randsize2[1])\n",
        "            padding_size_x_2 = random.randint(randsize2[0],randsize2[1])\n",
        "            padding_size_y_1 = random.randint(randsize2[0],randsize2[1])\n",
        "            padding_size_y_2 = random.randint(randsize2[0],randsize2[1])\n",
        "         \n",
        "            if(y - padding_size_y_1 > 0 and x - padding_size_x_1 > 0\n",
        "               and x + new_w + padding_size_x_2 < w and y + new_h + padding_size_y_2 < h):\n",
        "                padding_x_1 = padding_size_x_1\n",
        "                padding_x_2 = padding_size_x_2\n",
        "                padding_y_1 = padding_size_y_1\n",
        "                padding_y_2 = padding_size_y_2\n",
        "                \n",
        "            else:\n",
        "                padding_size_x_1 = random.randint(randsize3[0],randsize3[1])\n",
        "                padding_size_x_2 = random.randint(randsize3[0],randsize3[1])\n",
        "                padding_size_y_1 = random.randint(randsize3[0],randsize3[1])\n",
        "                padding_size_y_2 = random.randint(randsize3[0],randsize3[1])\n",
        "         \n",
        "                if(y - padding_size_y_1 > 0 and x - padding_size_x_1 > 0\n",
        "                   and x + new_w + padding_size_x_2 < w and y + new_h + padding_size_y_2 < h):\n",
        "                    padding_x_1 = padding_size_x_1\n",
        "                    padding_x_2 = padding_size_x_2\n",
        "                    padding_y_1 = padding_size_y_1\n",
        "                    padding_y_2 = padding_size_y_2 \n",
        "        \n",
        "        \n",
        "        image_copy = image_copy[y - padding_y_1: y + new_h + padding_y_2, x - padding_x_1: x + new_w + padding_x_2]     \n",
        "        \n",
        "        key_pts = key_pts - [x - padding_x_1, y - padding_y_1] \n",
        "        \n",
        "        return {'image': image_copy, 'keypoints': key_pts}\n",
        "    \n",
        "class FaceCropTight(object):\n",
        "    \"\"\" Crop out face using the keypoints as reference\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"       \n",
        "        \n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "\n",
        "        image_copy = np.copy(image)\n",
        "        \n",
        "        h, w = image.shape[:2]\n",
        "        \n",
        "        x_max = 0\n",
        "        x_min = 10000\n",
        "        y_max = 0\n",
        "        y_min = 10000\n",
        "        \n",
        "        # Find the coordinates to keypoints at the far left, far right, top and bottom\n",
        "        # Also check that no keypoints are outside the image\n",
        "        for coord in key_pts:\n",
        "            if coord[0] > x_max:\n",
        "                if coord[0] >= w:\n",
        "                    x_max = w\n",
        "                else:\n",
        "                    x_max = coord[0]\n",
        "            if coord[0] < x_min:\n",
        "                if coord[0] < 0:\n",
        "                    x_min = 0\n",
        "                else:\n",
        "                    x_min = coord[0]\n",
        "            if coord[1] > y_max:\n",
        "                if coord[1] >= h:\n",
        "                    y_max = h\n",
        "                else:\n",
        "                    y_max = coord[1]\n",
        "            if coord[1] < y_min:\n",
        "                if coord[1] < 0:\n",
        "                    y_min = 0\n",
        "                else:\n",
        "                    y_min = coord[1]\n",
        "        \n",
        "        # Set the the left corner keypoint as out crop cooridnate\n",
        "        x = int(x_min)\n",
        "        y = int(y_min)\n",
        "        \n",
        "        # Get height and width of keypoint area\n",
        "        new_h = int(y_max - y_min)\n",
        "        new_w = int(x_max - x_min)\n",
        "        \n",
        "        #Set the smallest side equal to the largest since we want a square\n",
        "        if new_h > new_w:\n",
        "            new_w = new_h\n",
        "        else:\n",
        "            new_h = new_w       \n",
        "        \n",
        "        randsize1 = [5, 10]\n",
        "\n",
        "        # Check that padding dosent go outside the frame\n",
        "        padding_x_1 = 0\n",
        "        padding_x_2 = 0\n",
        "        padding_y_1 = 0\n",
        "        padding_y_2 = 0\n",
        "        \n",
        "        padding_size_x_1 = random.randint(randsize1[0],randsize1[1])\n",
        "        padding_size_x_2 = random.randint(randsize1[0],randsize1[1])\n",
        "        padding_size_y_1 = random.randint(randsize1[0],randsize1[1])\n",
        "        padding_size_y_2 = random.randint(randsize1[0],randsize1[1])\n",
        "        \n",
        "        if(y - padding_size_y_1 > 0 and x - padding_size_x_1 > 0\n",
        "           and x + new_w + padding_size_x_2 < w and y + new_h + padding_size_y_2 < h):\n",
        "            padding_x_1 = padding_size_x_1\n",
        "            padding_x_2 = padding_size_x_2\n",
        "            padding_y_1 = padding_size_y_1\n",
        "            padding_y_2 = padding_size_y_2\n",
        "                 \n",
        "        image_copy = image_copy[y - padding_y_1: y + new_h + padding_y_2, x - padding_x_1: x + new_w + padding_x_2]     \n",
        "        \n",
        "        key_pts = key_pts - [x - padding_x_1, y - padding_y_1] \n",
        "        \n",
        "        return {'image': image_copy, 'keypoints': key_pts}\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "         \n",
        "        # if image has no grayscale color channel, add one\n",
        "        if(len(image.shape) == 2):\n",
        "            # add that third color dim\n",
        "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
        "            \n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'keypoints': torch.from_numpy(key_pts)}\n",
        "    \n",
        "class Random90DegFlip(object):\n",
        "    \"\"\"Random 90 degree flip of image in sample\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "        \n",
        "        image_copy = np.copy(image)\n",
        "        key_pts_copy = np.copy(key_pts)\n",
        "\n",
        "        if random.choice([0, 1]) < .25:\n",
        "            image_copy = np.rot90(image_copy,1)\n",
        "            image_copy = np.flipud(image_copy)\n",
        "            key_pts_copy = np.fliplr(key_pts_copy)\n",
        "            \n",
        "\n",
        "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
        "    \n",
        "class RandomGamma(object):\n",
        "    \"\"\"Random gamma of image in sample\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "        \n",
        "        image_copy = np.copy(image)\n",
        "        key_pts_copy = np.copy(key_pts)\n",
        "\n",
        "        image_copy = adjust_gamma(image_copy, gamma=random.uniform(0.8, 1.1)) \n",
        "        \n",
        "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
        "    \n",
        "class ColorJitter(object):\n",
        "    \"\"\"ColorJitter image in sample\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "        \n",
        "        color_jitter = transforms.ColorJitter(\n",
        "            brightness=0.4,\n",
        "            contrast=0.4,\n",
        "            saturation=0.4,)\n",
        "        \n",
        "        image_copy = np.copy(image)\n",
        "        \n",
        "        key_pts_copy = np.copy(key_pts)\n",
        "\n",
        "        image_copy = color_jitter(Image.fromarray(image_copy)) \n",
        "       \n",
        "        image_copy = np.array(image_copy)\n",
        "        \n",
        "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
        "    \n",
        "def adjust_gamma(image, gamma=1.0):# build a lookup table mapping the pixel values [0, 255] to \n",
        "    # their adjusted gamma values\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
        "    for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        " \n",
        "    # apply gamma correction using the lookup table\n",
        "    return cv2.LUT(image, table)\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    \"\"\"Random horizontal flip of image in sample\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "        \n",
        "        image_copy = np.copy(image)\n",
        "        key_pts_copy = np.copy(key_pts)\n",
        "        key_pts_copy_2 = np.copy(key_pts_copy)\n",
        "\n",
        "        if random.choice([0, 1]) <= 0.5:\n",
        "            # horizontally flip image\n",
        "            image_copy = np.fliplr(image_copy)          \n",
        "            # keypoints (x,y) = (-x,y)\n",
        "            key_pts_copy[:,0] = -key_pts_copy[:, 0]\n",
        "            # move keypoints form 2 kvadrant to 1 kvadrant\n",
        "            key_pts_copy[:,0] = key_pts_copy[:, 0] + image_copy.shape[1]\n",
        "\n",
        "            # since the keypoints are fliped around the y axis\n",
        "            # their placment are wrong int the keypoint array.\n",
        "            # E.g. the right eye and left eye is in the wrong place,\n",
        "            # so the keypoints need to be correctly mirrord in the list\n",
        "            \n",
        "            key_pts_copy_2 = np.copy(key_pts_copy)\n",
        "            \n",
        "            # mirror jawline \n",
        "            key_pts_copy_2[16] = key_pts_copy[0]\n",
        "            key_pts_copy_2[15] = key_pts_copy[1]\n",
        "            key_pts_copy_2[14] = key_pts_copy[2]\n",
        "            key_pts_copy_2[13] = key_pts_copy[3]\n",
        "            key_pts_copy_2[12] = key_pts_copy[4]\n",
        "            key_pts_copy_2[11] = key_pts_copy[5]\n",
        "            key_pts_copy_2[10] = key_pts_copy[6]\n",
        "            key_pts_copy_2[9]  = key_pts_copy[7]\n",
        "            key_pts_copy_2[8]  = key_pts_copy[8]\n",
        "            key_pts_copy_2[7] = key_pts_copy[9] \n",
        "            key_pts_copy_2[6] = key_pts_copy[10] \n",
        "            key_pts_copy_2[5] = key_pts_copy[11]\n",
        "            key_pts_copy_2[4] = key_pts_copy[12]\n",
        "            key_pts_copy_2[3] = key_pts_copy[13]\n",
        "            key_pts_copy_2[2] = key_pts_copy[14]\n",
        "            key_pts_copy_2[1] = key_pts_copy[15]\n",
        "            key_pts_copy_2[0]  = key_pts_copy[16]\n",
        "            \n",
        "            # mirror eyebrowns\n",
        "            key_pts_copy_2[26] = key_pts_copy[17] \n",
        "            key_pts_copy_2[25] = key_pts_copy[18] \n",
        "            key_pts_copy_2[24] = key_pts_copy[19]\n",
        "            key_pts_copy_2[23] = key_pts_copy[20]\n",
        "            key_pts_copy_2[22] = key_pts_copy[21]\n",
        "            key_pts_copy_2[21] = key_pts_copy[22]\n",
        "            key_pts_copy_2[20] = key_pts_copy[23]\n",
        "            key_pts_copy_2[19] = key_pts_copy[24]\n",
        "            key_pts_copy_2[18] = key_pts_copy[25] \n",
        "            key_pts_copy_2[17] = key_pts_copy[26]\n",
        "            \n",
        "            # mirror nose tip\n",
        "            key_pts_copy_2[35] = key_pts_copy[31] \n",
        "            key_pts_copy_2[34] = key_pts_copy[32] \n",
        "            key_pts_copy_2[33] = key_pts_copy[33]\n",
        "            key_pts_copy_2[32] = key_pts_copy[34]\n",
        "            key_pts_copy_2[31] = key_pts_copy[35]\n",
        "            \n",
        "            # mirror eyes\n",
        "            key_pts_copy_2[45] = key_pts_copy[36]\n",
        "            key_pts_copy_2[44] = key_pts_copy[37] \n",
        "            key_pts_copy_2[43] = key_pts_copy[38]\n",
        "            key_pts_copy_2[42] = key_pts_copy[39]\n",
        "            key_pts_copy_2[47] = key_pts_copy[40]\n",
        "            key_pts_copy_2[46] = key_pts_copy[41] \n",
        "            key_pts_copy_2[39] = key_pts_copy[42] \n",
        "            key_pts_copy_2[38] = key_pts_copy[43]\n",
        "            key_pts_copy_2[37] = key_pts_copy[44]\n",
        "            key_pts_copy_2[36] = key_pts_copy[45]\n",
        "            key_pts_copy_2[41] = key_pts_copy[46] \n",
        "            key_pts_copy_2[40] = key_pts_copy[47] \n",
        "   \n",
        "            # mirror lips\n",
        "            key_pts_copy_2[54] = key_pts_copy[48]\n",
        "            key_pts_copy_2[53] = key_pts_copy[49] \n",
        "            key_pts_copy_2[52] = key_pts_copy[50]\n",
        "            key_pts_copy_2[51] = key_pts_copy[51]\n",
        "            key_pts_copy_2[50] = key_pts_copy[52]\n",
        "            key_pts_copy_2[49] = key_pts_copy[53] \n",
        "            key_pts_copy_2[48] = key_pts_copy[54]\n",
        "            \n",
        "            key_pts_copy_2[59] = key_pts_copy[55]\n",
        "            key_pts_copy_2[58] = key_pts_copy[56]\n",
        "            key_pts_copy_2[57] = key_pts_copy[57]\n",
        "            key_pts_copy_2[56] = key_pts_copy[58] \n",
        "            key_pts_copy_2[55] = key_pts_copy[59]\n",
        "            \n",
        "            key_pts_copy_2[64] = key_pts_copy[60]\n",
        "            key_pts_copy_2[63] = key_pts_copy[61] \n",
        "            key_pts_copy_2[62] = key_pts_copy[62]\n",
        "            key_pts_copy_2[61] = key_pts_copy[63]\n",
        "            key_pts_copy_2[60] = key_pts_copy[64]\n",
        "                 \n",
        "            key_pts_copy_2[67] = key_pts_copy[65] \n",
        "            key_pts_copy_2[66] = key_pts_copy[66] \n",
        "            key_pts_copy_2[65] = key_pts_copy[67]\n",
        "\n",
        "            \n",
        "        return {'image': image_copy, 'keypoints': key_pts_copy_2}\n",
        "\n",
        "# inspired by https://github.com/macbrennan90/facial-keypoint-detection/blob/master/CV_project.ipynb and\n",
        "# https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html\n",
        "class Rotate(object):\n",
        "    \"\"\"Rotate image in sample by an angle\"\"\"\n",
        "    \n",
        "    def __init__(self, rotation):\n",
        "        self.rotation = rotation\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "        \n",
        "        image_copy = np.copy(image)\n",
        "        key_pts_copy = np.copy(key_pts)\n",
        "        \n",
        "        rows = image.shape[0]\n",
        "        cols = image.shape[1]\n",
        "        \n",
        "        M = cv2.getRotationMatrix2D((rows/2,cols/2),90,1)\n",
        "        image_copy = cv2.warpAffine(image_copy,M,(cols,rows))\n",
        "                \n",
        "        \n",
        "        key_pts_copy = key_pts_copy.reshape((1,136))\n",
        "        new_keypoints = np.zeros(136)\n",
        "        \n",
        "        for i in range(68):\n",
        "            coord_idx = 2*i\n",
        "            old_coord = key_pts_copy[0][coord_idx:coord_idx+2]\n",
        "            new_coord = np.matmul(M,np.append(old_coord,1))\n",
        "            new_keypoints[coord_idx] += new_coord[0]\n",
        "            new_keypoints[coord_idx+1] += new_coord[1]\n",
        "        \n",
        "        new_keypoints = new_keypoints.reshape((68,2))\n",
        "        \n",
        "        return {'image': image_copy, 'keypoints': new_keypoints}\n",
        "    \n",
        "class RandomRotate(object):\n",
        "    \"\"\"Rotate image in sample by an angle\"\"\"\n",
        "    \n",
        "    def __init__(self, rotation=30):\n",
        "        self.rotation = rotation\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "        \n",
        "        image_copy = np.copy(image)\n",
        "        key_pts_copy = np.copy(key_pts)\n",
        "        \n",
        "        rows = image.shape[0]\n",
        "        cols = image.shape[1]\n",
        "        \n",
        "        M = cv2.getRotationMatrix2D((rows/2,cols/2),random.choice([-self.rotation, self.rotation]),1)\n",
        "        image_copy = cv2.warpAffine(image_copy,M,(cols,rows))\n",
        "                \n",
        "        \n",
        "        key_pts_copy = key_pts_copy.reshape((1,136))\n",
        "        new_keypoints = np.zeros(136)\n",
        "        \n",
        "        for i in range(68):\n",
        "            coord_idx = 2*i\n",
        "            old_coord = key_pts_copy[0][coord_idx:coord_idx+2]\n",
        "            new_coord = np.matmul(M,np.append(old_coord,1))\n",
        "            new_keypoints[coord_idx] += new_coord[0]\n",
        "            new_keypoints[coord_idx+1] += new_coord[1]\n",
        "        \n",
        "        new_keypoints = new_keypoints.reshape((68,2))\n",
        "        \n",
        "        return {'image': image_copy, 'keypoints': new_keypoints}"
      ],
      "metadata": {
        "id": "-n0n5aOSFQtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(csv_path, split):\n",
        "    df_data = pd.read_csv(csv_path)\n",
        "    len_data = len(df_data)\n",
        "    # calculate the validation data sample length\n",
        "    valid_split = int(len_data * split)\n",
        "    # calculate the training data samples length\n",
        "    train_split = int(len_data - valid_split)\n",
        "    training_samples = df_data.iloc[:train_split][:]\n",
        "    valid_samples = df_data.iloc[-valid_split:][:]\n",
        "    # print(training_samples)\n",
        "    return training_samples, valid_samples"
      ],
      "metadata": {
        "id": "gp1kqJFYFUF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the training and validation data samples\n",
        "training_samples, valid_samples = train_test_split(f\"{ROOT_PATH}/training_frames_keypoints.csv\",\n",
        "                                                   0.1)\n",
        "\n",
        "\n",
        "data_transform_1 = transforms.Compose([Rescale(250),\n",
        "                                     RandomCrop(224),\n",
        "                                     Normalize(),\n",
        "                                     ToTensor()])\n",
        "\n",
        "data_transform_2 = transforms.Compose([RandomRotate(5),\n",
        "                                     RandomHorizontalFlip(),\n",
        "                                     ColorJitter(),\n",
        "                                     FaceCrop(),\n",
        "                                     Rescale((224,224)),\n",
        "                                     Normalize(),\n",
        "                                     ToTensor()])\n",
        "\n",
        "transform_resnet = transforms.Compose([Rescale((224,224)),\n",
        "                                       ToTensor()])"
      ],
      "metadata": {
        "id": "rfK3m_esFXCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the dataset - `FaceKeypointDataset()`\n",
        "train_data = FaceKeypointDataset(training_samples, \n",
        "                                 f\"{ROOT_PATH}/training\",\n",
        "                                 transform=data_transform_2)\n",
        "valid_data = FaceKeypointDataset(valid_samples, \n",
        "                                 f\"{ROOT_PATH}/training\",\n",
        "                                 transform=data_transform_2)\n",
        "\n",
        "\n",
        "# prepare data loaders\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_data, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          shuffle=False)\n",
        "print(f\"Training sample instances: {len(train_data)}\")\n",
        "print(f\"Validation sample instances: {len(valid_data)}\")"
      ],
      "metadata": {
        "id": "gDY4WUWuFZQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a few of the images from the dataset\n",
        "num_to_display = 3\n",
        "\n",
        "for i in range(num_to_display):\n",
        "    \n",
        "    # define the size of images\n",
        "    fig = plt.figure(figsize=(20,10))\n",
        "    \n",
        "    # randomly select a sample\n",
        "    rand_i = np.random.randint(0, len(train_data))\n",
        "    sample = train_data[rand_i]\n",
        "\n",
        "    # print the shape of the image and keypoints\n",
        "    print(i, sample['image'].shape, sample['keypoints'].shape)\n",
        "\n",
        "    ax = plt.subplot(1, num_to_display, i + 1)\n",
        "    ax.set_title('Sample #{}'.format(i))\n",
        "    \n",
        "    # Using the same display function, defined earlier\n",
        "    show_keypoints(sample['image'], sample['keypoints'])"
      ],
      "metadata": {
        "id": "JPustEeBFbYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KdlPhF6gFg_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow-object-detection-api"
      ],
      "metadata": {
        "id": "FRF5F6YiFkZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# import the usual resources\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# import utilities to keep workspaces alive during model training\n",
        "# from workspace_utils import active_session\n",
        "\n",
        "# watch for any changes in model.py, if it changes, re-load it automatically\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "lEPJh17aFmD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the convolutional neural network architecture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# can use the below import should you choose to initialize the weights of your Net\n",
        "import torch.nn.init as I\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        ## Define all the layers of this CNN, the only requirements are:\n",
        "        ## 1. This network takes in a square (same width and height), grayscale image as input\n",
        "        ## 2. It ends with a linear layer that represents the keypoints\n",
        "        ## Last layer output 136 values, 2 for each of the 68 keypoint (x, y) pairs\n",
        "        \n",
        "        # 1 input image channel (grayscale), 32 output channels/feature maps, 5x5 square convolution kernel\n",
        "        \n",
        "        ## Shape of a Convolutional Layer\n",
        "        # K - out_channels : the number of filters in the convolutional layer\n",
        "        # F - kernel_size\n",
        "        # S - the stride of the convolution\n",
        "        # P - the padding\n",
        "        # W - the width/height (square) of the previous layer\n",
        "        \n",
        "        # Since there are F*F*D weights per filter\n",
        "        # The total number of weights in the convolutional layer is K*F*F*D\n",
        "        \n",
        "        # 224 by 224 pixels\n",
        "        \n",
        "        ## self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
        "        # output size = (W-F)/S +1 = (224-5)/1 +1 = 220\n",
        "        # the output Tensor for one image, will have the dimensions: (1, 220, 220)\n",
        "        # after one pool layer, this becomes (10, 13, 13)\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
        "        \n",
        "        # maxpool layer\n",
        "        # pool with kernel_size=2, stride=2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # 220/2 = 110\n",
        "        # output size = (W-F)/S +1 = (110-3)/1 + 1 = 108\n",
        "        # the output Tensor for one image, will have the dimensions: (32, 110, 110)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        \n",
        "        # output size = (W-F)/S +1 = (54-3)/1 + 1 = 52\n",
        "        # the output Tensor for one image, will have the dimensions: (64, 54, 54)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "        \n",
        "        # output size = (W-F)/S +1 = (26-3)/1 + 1 = 24\n",
        "        # the output Tensor for one image, will have the dimensions: (128, 26, 26)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3)\n",
        "        \n",
        "        # output size = (W-F)/S +1 = (12-3)/1 + 1 = 10\n",
        "        # the output Tensor for one image, will have the dimensions: (256, 12, 12)\n",
        "        self.conv5 = nn.Conv2d(256, 512, 1)\n",
        "        \n",
        "        # output size = (W-F)/S +1 = (6-1)/1 + 1 = 6\n",
        "        # the output Tensor for one image, will have the dimensions: (512, 6, 6)\n",
        "        \n",
        "        # Fully-connected (linear) layers\n",
        "        self.fc1 = nn.Linear(512*6*6, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 68*2)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        ## Define the feedforward behavior of this model\n",
        "        ## x is the input image and, as an example, here you may choose to include a pool/conv step:\n",
        "        \n",
        "        # 5 conv/relu + pool layers\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "        \n",
        "        # Prep for linear layer / Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # linear layers with dropout in between\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "7cr_LbZPFqxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pretrainedmodels\n",
        "import pretrainedmodels\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "vS2Gb7_xFr24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceKeypointResNet50(nn.Module):\n",
        "    def __init__(self, pretrained, requires_grad):\n",
        "        super(FaceKeypointResNet50, self).__init__()\n",
        "        if pretrained == True:\n",
        "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained='imagenet')\n",
        "        else:\n",
        "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained=None)\n",
        "\n",
        "\n",
        "        if requires_grad == True:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = True\n",
        "            print('Training intermediate layer parameters...')\n",
        "        elif requires_grad == False:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "            print('Freezing intermediate layer parameters...')\n",
        "\n",
        "        # change the final layer\n",
        "        self.l0 = nn.Linear(2048, 136)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get the batch size only, ignore (c, h, w)\n",
        "        batch, _, _, _ = x.shape\n",
        "        x = self.model.features(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
        "        l0 = self.l0(x)\n",
        "        return l0"
      ],
      "metadata": {
        "id": "7-4ndmlzFxB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class resnet18_grayscale(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(resnet18_grayscale, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=True)\n",
        "        # change from supporting color to gray scale images\n",
        "        self.resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        n_inputs = self.resnet18.fc.in_features\n",
        "        self.resnet18.fc = nn.Linear(n_inputs, 136)\n",
        "                        \n",
        "    def forward(self, x):\n",
        "        x = self.resnet18(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Fyf_GndsF0Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# *** Conv2d output dimensions ***\n",
        "# height_out = (height_in + 2*padding - dilation*(kernel_size - 1) - 1)/stride + 1\n",
        "# width_out = (width_in + 2*padding - dilation*(kernel_size - 1) - 1)/stride + 1\n",
        "# weights_out = height_out * width_out * channels_out\n",
        "#\n",
        "# With values: strid = 1, padding = 0, dilation = 1\n",
        "# height_out = height_in - kernel_size + 1\n",
        "# width_out = width_in - kernel_size + 1\n",
        "#\n",
        "# *** MaxPool2d output dimensions ***\n",
        "# height_out = (height_in + 2*padding - dilation*(kernel_size - 1) - 1)/stride + 1\n",
        "# width_out = (width_in + 2*padding - dilation*(kernel_size - 1) - 1)/stride + 1\n",
        "# weights_out = height_out * width_out * channels_out\n",
        "#\n",
        "# With values: strid = 2, padding = 0, dilation = 1\n",
        "# height_out = (height_in - kernel_size)/2 + 1\n",
        "# width_out = (width_in - kernel_size)/2 + 1\n",
        "\n",
        "class NaimishNet(nn.Module):\n",
        "    def __init__(self, image_size, output_size = 136, kernels = [5,5,5,5],out_channels = [32,64,128,256],\n",
        "                dropout_p = [0, 0, 0, 0, 0, 0], use_padding=True, use_maxp = True):\n",
        "        super(NaimishNet, self).__init__() \n",
        "        # padding only support odd numbered kernels in this implementation\n",
        "        self.use_padding = use_padding\n",
        "        \n",
        "        # init padding\n",
        "        if self.use_padding:\n",
        "            self.padding = [int((k-1)/2) for k in kernels]\n",
        "        else:\n",
        "            self.padding = [0,0,0,0]\n",
        "            \n",
        "        # Find the size of the last maxp output. \n",
        "        last_maxp_size = image_size\n",
        "        for idx, val in enumerate(kernels):\n",
        "            if self.use_padding:\n",
        "                last_maxp_size = last_maxp_size//2\n",
        "            else:\n",
        "                last_maxp_size = (last_maxp_size - (val-1))//2\n",
        "        last_maxp_size = out_channels[3] * last_maxp_size * last_maxp_size\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            OrderedDict([\n",
        "            ('conv1', nn.Conv2d(1, out_channels[0], kernel_size=kernels[0], padding=self.padding[0])),\n",
        "            ('relu1', nn.ReLU())\n",
        "            ])) # (32, 252, 252)                        \n",
        "        \n",
        "        if use_maxp:\n",
        "            self.maxp1 = nn.Sequential(OrderedDict([\n",
        "                ('maxp1', nn.MaxPool2d(2, 2)),\n",
        "                ('dropout1', nn.Dropout(dropout_p[0])),\n",
        "                ('bachnorm1', nn.BatchNorm2d(out_channels[0]))\n",
        "                ])) # (32, 126, 126)\n",
        "        else:\n",
        "            self.maxp1 = nn.Sequential(OrderedDict([\n",
        "                ('maxp1', nn.AvgPool2d(2, 2)),\n",
        "                ('dropout1', nn.Dropout(dropout_p[0])),\n",
        "                ('bachnorm1', nn.BatchNorm2d(out_channels[0]))\n",
        "                ])) # (32, 126, 126)\n",
        "\n",
        "        self.conv2 = nn.Sequential(OrderedDict([\n",
        "            ('conv2', nn.Conv2d(out_channels[0], out_channels[1], kernel_size=kernels[1], padding=self.padding[1])),\n",
        "            ('relu2', nn.ReLU())\n",
        "            ])) # (64, 122, 122)\n",
        "        \n",
        "        if use_maxp:\n",
        "            self.maxp2 = nn.Sequential(OrderedDict([\n",
        "                ('maxp2', nn.MaxPool2d(2, 2)),\n",
        "                ('dropout2', nn.Dropout(dropout_p[1])),\n",
        "                ('bachnorm2', nn.BatchNorm2d(out_channels[1]))\n",
        "                ])) # (64, 61, 61)\n",
        "        else:\n",
        "            self.maxp2 = nn.Sequential(OrderedDict([\n",
        "                ('maxp2', nn.AvgPool2d(2, 2)),\n",
        "                ('dropout2', nn.Dropout(dropout_p[1])),\n",
        "                ('bachnorm2', nn.BatchNorm2d(out_channels[1]))\n",
        "                ])) # (64, 61, 61)\n",
        "            \n",
        "        self.conv3 = nn.Sequential(OrderedDict([\n",
        "            ('conv3', nn.Conv2d(out_channels[1], out_channels[2], kernel_size=kernels[2], padding=self.padding[2])),\n",
        "            ('relu3', nn.ReLU())\n",
        "            ])) # (128, 59, 59)\n",
        "\n",
        "        if use_maxp:\n",
        "            self.maxp3 = nn.Sequential(OrderedDict([\n",
        "                ('maxp3', nn.MaxPool2d(2, 2)),\n",
        "                ('dropout3', nn.Dropout(dropout_p[2])),\n",
        "                ('bachnorm3', nn.BatchNorm2d(out_channels[2]))\n",
        "                ])) # (128, 29, 29)\n",
        "        else:\n",
        "            self.maxp3 = nn.Sequential(OrderedDict([\n",
        "                ('maxp3', nn.AvgPool2d(2, 2)),\n",
        "                ('dropout3', nn.Dropout(dropout_p[2])),\n",
        "                ('bachnorm3', nn.BatchNorm2d(out_channels[2]))\n",
        "                ])) # (128, 29, 29)\n",
        "            \n",
        "        self.conv4 = nn.Sequential(OrderedDict([\n",
        "            ('conv4', nn.Conv2d(out_channels[2], out_channels[3], kernel_size=kernels[3], padding=self.padding[3])),\n",
        "            ('relu4', nn.ReLU())\n",
        "            ])) # (256, 27, 27)\n",
        "        \n",
        "        if use_maxp:\n",
        "            self.maxp4 = nn.Sequential(OrderedDict([\n",
        "                ('maxp4', nn.MaxPool2d(2, 2)),\n",
        "                ('dropout4', nn.Dropout(dropout_p[3])),\n",
        "                ('bachnorm4', nn.BatchNorm2d(out_channels[3]))\n",
        "                ]))  # (256, 13, 13)\n",
        "        else:\n",
        "            self.maxp4 = nn.Sequential(OrderedDict([\n",
        "                ('maxp4', nn.AvgPool2d(2, 2)),\n",
        "                ('dropout4', nn.Dropout(dropout_p[3])),\n",
        "                ('bachnorm4', nn.BatchNorm2d(out_channels[3]))\n",
        "                ]))  # (256, 13, 13)\n",
        "        \n",
        "        self.fc1 = nn.Sequential(OrderedDict([\n",
        "            ('fc1', nn.Linear(last_maxp_size, 1024)),\n",
        "            ('relu5', nn.ReLU()),\n",
        "            ('dropout5', nn.Dropout(dropout_p[4])),\n",
        "            ('bachnorm5', nn.BatchNorm1d(1024))\n",
        "            ])) # (36864, 1024)\n",
        "\n",
        "        self.fc2 = nn.Sequential(OrderedDict([\n",
        "            ('fc2', nn.Linear(1024, 1024)),\n",
        "            ('relu6', nn.ReLU()),\n",
        "            ('dropout6', nn.Dropout(dropout_p[5])),\n",
        "            ('bachnorm6', nn.BatchNorm1d(1024))\n",
        "            ])) # (1024, 1024)\n",
        "\n",
        "        self.fc3 = nn.Sequential(OrderedDict([\n",
        "            ('fc3', nn.Linear(1024, output_size))\n",
        "            ])) # (1024, 136)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print('before conv1')\n",
        "        # print(x.size())\n",
        "        out = self.conv1(x)\n",
        "        # print('conv1')\n",
        "        # print(out.size())\n",
        "        out = self.maxp1(out)\n",
        "        # print('maxp1')\n",
        "        # print(out.size())\n",
        "        out = self.conv2(out)\n",
        "        # print('conv2')\n",
        "        # print(out.size())\n",
        "        out = self.maxp2(out)\n",
        "        # print('maxp2')\n",
        "        # print(out.size())\n",
        "        out = self.conv3(out)\n",
        "        # print('conv3')\n",
        "        # print(out.size())\n",
        "        out = self.maxp3(out)\n",
        "        # print('maxp3')\n",
        "        # print(out.size())\n",
        "        out = self.conv4(out)\n",
        "        # print('conv4')\n",
        "        # print(out.size())\n",
        "        out = self.maxp4(out)\n",
        "        # print('maxp4')\n",
        "        # print(out.size())\n",
        "        out = out.view(out.size(0), -1)\n",
        "        # print('After out.view')\n",
        "        # print(out.size())\n",
        "        out = self.fc1(out)\n",
        "        # print('fc1')\n",
        "        # print(out.size())\n",
        "        out = self.fc2(out)\n",
        "        # print('fc2')\n",
        "        # print(out.size())\n",
        "        out = self.fc3(out)\n",
        "        # print('fc3')\n",
        "        # print(out.size())\n",
        "        # print('done')\n",
        "        return out\n",
        "    \n",
        "    def __str__(self):\n",
        "        pretty_net_str = ''\n",
        "        for layer_name in self._modules:\n",
        "            pretty_net_str += f'{layer_name}:\\n'\n",
        "            for items in getattr(self, layer_name):\n",
        "                pretty_net_str += f'{items}\\n'\n",
        "            pretty_net_str += '\\n'\n",
        "        return pretty_net_str\n"
      ],
      "metadata": {
        "id": "kZBV3EI6F0QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the Net in models.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## Once you've define the network, you can instantiate it\n",
        "# one example conv layer has been provided for you\n",
        "#from models import Net\n",
        "\n",
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "naimish = NaimishNet(224)\n",
        "print(naimish)"
      ],
      "metadata": {
        "id": "CPLmiTXlF7O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in the test data, using the dataset class\n",
        "# AND apply the data_transform you defined above\n",
        "\n",
        "# create the test dataset\n",
        "# test_dataset = FacialKeypointsDataset(csv_file='/content/drive/MyDrive/Facial-Keypoint-Detection-master/data/test_frames_keypoints.csv',\n",
        "                                            #  root_dir='/content/drive/MyDrive/Facial-Keypoint-Detection-master/data/test',\n",
        "                                            #  transform=data_transform)\n",
        "\n",
        "test_samples, waste = train_test_split(f\"{ROOT_PATH}/test_frames_keypoints.csv\",\n",
        "                                                   0.0)\n",
        "\n",
        "test_data_transform = transforms.Compose([FaceCrop(),\n",
        "                                          Rescale((224,224)),\n",
        "                                          Normalize(),\n",
        "                                          ToTensor()])\n",
        "\n",
        "test_data = FaceKeypointDataset(test_samples, \n",
        "                                f\"{ROOT_PATH}/test\",\n",
        "                                transform=test_data_transform)\n",
        "\n",
        "\n",
        "# load test data in batches\n",
        "batch_size = 16\n",
        "\n",
        "test_loader = DataLoader(test_data, \n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)"
      ],
      "metadata": {
        "id": "mi95xLtPF-hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model on a batch of test images\n",
        "\n",
        "def net_sample_output(model):\n",
        "    \n",
        "    for i, sample in enumerate(test_loader):\n",
        "  \n",
        "        images = sample['image'].to(DEVICE)\n",
        "        key_pts = sample['keypoints'].to(DEVICE)\n",
        "        \n",
        "        # convert images to FloatTensors\n",
        "        images = images.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        # forward pass to get net output\n",
        "        output_pts = model(images)\n",
        "\n",
        "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
        "       \n",
        "        if i == 0:\n",
        "            return images, output_pts, key_pts\n",
        "            "
      ],
      "metadata": {
        "id": "sN2wzsqzGBPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the above function\n",
        "# returns: test images, test predicted keypoints, test ground truth keypoints\n",
        "test_images, test_outputs, gt_pts = net_sample_output(net.to(DEVICE))\n",
        "\n",
        "\n",
        "# print out the dimensions of the data to see if they make sense\n",
        "print(test_images.data.size())\n",
        "print(test_outputs.data.size())\n",
        "print(gt_pts.size())"
      ],
      "metadata": {
        "id": "qyKji1snGDTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n",
        "    \"\"\"Show image with predicted keypoints\"\"\"\n",
        "    \n",
        "    # image is grayscale\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n",
        "    # plot ground truth points as green pts\n",
        "    if gt_pts is not None:\n",
        "        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n"
      ],
      "metadata": {
        "id": "1RRq020CGFg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the output\n",
        "# by default this shows a batch of 10 images\n",
        "def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for i in range(10):    \n",
        "        ax = plt.subplot(3, 5, i+1)\n",
        "\n",
        "        # un-transform the image data\n",
        "        image = test_images[i].data.detach().cpu()    # get the image from it's wrapper\n",
        "        image = image.numpy()   # convert to numpy array from a Tensor\n",
        "        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n",
        "\n",
        "        # un-transform the predicted key_pts data\n",
        "        predicted_key_pts = test_outputs[i].data.detach().cpu() \n",
        "        predicted_key_pts = predicted_key_pts.numpy()\n",
        "        # undo normalization of keypoints  \n",
        "        predicted_key_pts = predicted_key_pts*(image.shape[0]/4)+image.shape[0]/2\n",
        "        \n",
        "        # plot ground truth points for comparison, if they exist\n",
        "        ground_truth_pts = None\n",
        "        if gt_pts is not None:\n",
        "            ground_truth_pts = gt_pts[i].detach().cpu()        \n",
        "            ground_truth_pts = ground_truth_pts*(image.shape[0]/4)+image.shape[0]/2\n",
        "        \n",
        "        # call show_all_keypoints\n",
        "        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n",
        "            \n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "# call it\n",
        "visualize_output(test_images, test_outputs, gt_pts)"
      ],
      "metadata": {
        "id": "rNr24V6MGIDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.style.use('ggplot')"
      ],
      "metadata": {
        "id": "_Dy4f1MAGXMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the loss and optimization\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = LR)"
      ],
      "metadata": {
        "id": "yUMGDSaRGUTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training function\n",
        "def fit(model, dataloader, data):\n",
        "    print('Training')\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    counter = 0\n",
        "    # calculate the number of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    for i, data in tqdm(enumerate(dataloader), total=num_batches):\n",
        "        counter += 1\n",
        "        image, keypoints = data['image'].to(DEVICE), data['keypoints'].to(DEVICE)\n",
        "        image, keypoints = image.type(torch.cuda.FloatTensor), keypoints.type(torch.cuda.FloatTensor)\n",
        "        # flatten the keypoints\n",
        "        keypoints = keypoints.view(keypoints.size(0), -1)\n",
        "        outputs = model(image)\n",
        "        loss = criterion(outputs, keypoints)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_running_loss += loss.item()\n",
        "\n",
        "    train_loss = train_running_loss/counter\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "qW41a9nIGY50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validatioon function\n",
        "def validate(model, dataloader, data, epoch):\n",
        "    print('Validating')\n",
        "    model.eval()\n",
        "    valid_running_loss = 0.0\n",
        "    counter = 0\n",
        "    # calculate the number of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(dataloader), total=num_batches):\n",
        "            counter += 1\n",
        "            image, keypoints = data['image'].to(DEVICE), data['keypoints'].to(DEVICE)\n",
        "            image, keypoints = image.type(torch.cuda.FloatTensor), keypoints.type(torch.cuda.FloatTensor)\n",
        "            # flatten the keypoints\n",
        "            keypoints = keypoints.view(keypoints.size(0), -1)\n",
        "            outputs = model(image)\n",
        "            loss = criterion(outputs, keypoints)\n",
        "            valid_running_loss += loss.item()\n",
        "        \n",
        "    valid_loss = valid_running_loss/counter\n",
        "    return valid_loss"
      ],
      "metadata": {
        "id": "NrgWmUkLGchV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execution(model):\n",
        "    global train_loss\n",
        "    train_loss = []\n",
        "    global val_loss\n",
        "    val_loss = []\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"Epoch {epoch+1} of {EPOCHS}\")\n",
        "        train_epoch_loss = fit(model, train_loader, train_data)\n",
        "        val_epoch_loss = validate(model, valid_loader, valid_data, epoch)\n",
        "        train_loss.append(train_epoch_loss)\n",
        "        val_loss.append(val_epoch_loss)\n",
        "        print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "        print(f'Val Loss: {val_epoch_loss:.4f}')"
      ],
      "metadata": {
        "id": "mQLwZ0hDGej6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss plots\n",
        "def plot_and_save(model, model_name = \"\"):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(train_loss, color='orange', label='train loss')\n",
        "    plt.plot(val_loss, color='red', label='validataion loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"/content/drive/MyDrive/Facial-Keypoint-Detection-master/saved_models/loss_{model_name}.png\")\n",
        "    plt.show()\n",
        "    torch.save({\n",
        "                'epoch': EPOCHS,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, f\"/content/drive/MyDrive/Facial-Keypoint-Detection-master/saved_models/{model_name}.pt\")\n",
        "    print('DONE TRAINING')"
      ],
      "metadata": {
        "id": "-gtcTclgGhQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "# load in color image for face detection\n",
        "image_1 = cv2.imread('/content/drive/MyDrive/Facial-Keypoint-Detection-master/images/Shiv_1.jpeg')\n",
        "image_2 = cv2.imread('/content/drive/MyDrive/Facial-Keypoint-Detection-master/images/Shiv_2.jpeg')\n",
        "image_3 = cv2.imread('/content/drive/MyDrive/Facial-Keypoint-Detection-master/images/Shiv_3.JPG')\n",
        "\n",
        "# switch red and blue color channels \n",
        "# --> by default OpenCV assumes BLUE comes first, not RED as in many images\n",
        "image_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2RGB)\n",
        "image_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2RGB)\n",
        "image_3 = cv2.cvtColor(image_3, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "images = [image_1, image_2, image_3]\n",
        "gray = images.copy()\n",
        "\n",
        "# plot the image\n",
        "fig = plt.figure(figsize=(9,9))\n",
        "plt.imshow(images[2])"
      ],
      "metadata": {
        "id": "09Bm2Pr7GjD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in a haar cascade classifier for detecting frontal faces\n",
        "face_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/Facial-Keypoint-Detection-master/detector_architectures/haarcascade_frontalface_default.xml')\n",
        "\n",
        "\n",
        "# run the detector\n",
        "# the output here is an array of detections; the corners of each detection box\n",
        "# if necessary, modify these parameters until you successfully identify every face in a given image\n",
        "\n",
        "faces_list = []\n",
        "images_with_detections = []\n",
        "# make a copy of the original image to plot detections on\n",
        "for image in images:\n",
        "    faces_list.append(face_cascade.detectMultiScale(image, 1.3, 5))\n",
        "    images_with_detections.append(image.copy())\n",
        "\n",
        "for i, faces in enumerate(faces_list):\n",
        "    # loop over the detected faces, mark the image where each face is found\n",
        "    \n",
        "    rec_thickness = images_with_detections[i].shape[0]//150\n",
        "    for (x,y,w,h) in faces:\n",
        "        # draw a rectangle around each detected face\n",
        "        # you may also need to change the width of the rectangle drawn depending on image resolution\n",
        "        cv2.rectangle(images_with_detections[i],(x,y),(x+w,y+h),(255,0,0),rec_thickness) \n",
        "        \n",
        "fig = plt.figure(figsize=(9,9))\n",
        "\n",
        "plt.imshow(images_with_detections[0])"
      ],
      "metadata": {
        "id": "gxiSHpuGGtC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "EPOCHS = 30\n",
        "# model_1_batch64_epochs100_transform2 = Net()\n",
        "\n",
        "## load the best saved model parameters (by your path name)\n",
        "## You'll need to un-comment the line below and add the correct name for *your* saved model\n",
        "# naimishNet_batch16_epochs30_smoothloss_transform1_lr001.load_state_dict(torch.load('/content/drive/MyDrive/Facial-Keypoint-Detection-master/saved_models/naimishNet_batch16_epochs30_mseloss_transform2.pt'), strict=False)\n",
        "resnet18 = resnet18_grayscale().to(DEVICE)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/Facial-Keypoint-Detection-master/saved_models/resnet18gray_batch16_epochs30_smoothloss_transform2.pt')\n",
        "resnet18.load_state_dict(checkpoint['model_state_dict'])\n",
        "resnet18.eval()\n",
        "\n",
        "## print out your net and prepare it for testing (uncomment the line below)\n",
        "# naimishNet_batch16_epochs30_smoothloss_transform1_lr001.eval()"
      ],
      "metadata": {
        "id": "6swnvegXGuPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_all_keypoints(image, keypoints):  \n",
        "    batch_size = len(image)\n",
        "    for i, face in enumerate(image):\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        ax = plt.subplot(1, batch_size, i+1)\n",
        "\n",
        "        # un-transform the predicted key_pts data\n",
        "        predicted_keypoints = keypoints[i].data\n",
        "        predicted_keypoints = predicted_keypoints.numpy()\n",
        "        # undo normalization of keypoints  \n",
        "        predicted_keypoints = predicted_keypoints*50.0+100\n",
        "\n",
        "        plt.imshow(face, cmap='gray')\n",
        "        plt.scatter(predicted_keypoints[:, 0], predicted_keypoints[:, 1], s=20, marker='.', c='m')\n",
        "        \n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9luMbN2LG1J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " img_size = 224\n",
        "\n",
        " def detect_keypoints(image_nr, scale, net):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    i = 0   \n",
        "    image_copy = np.copy(images[image_nr])\n",
        "    # loop over the detected faces from your haar cascade\n",
        "    for (x,y,w,h) in faces_list[image_nr]:\n",
        "    \n",
        "        # Select the region of interest that is the face in the image \n",
        "        roi = image_copy[y-scale:y+h+scale, x-scale:x+w+scale]\n",
        "        \n",
        "        ## Convert the face region from RGB to grayscale\n",
        "        roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)\n",
        "        ## Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]\n",
        "        roi = roi/255.0\n",
        "        ## Rescale the detected face to be the expected square size for your CNN (224x224, suggested)\n",
        "        h, w = roi.shape\n",
        "                \n",
        "        roi = cv2.resize(roi, (img_size, img_size))\n",
        "        \n",
        "        # Make copy for displaying keypoint over\n",
        "        roi_copy = np.copy(roi)\n",
        "        \n",
        "        ## Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
        "        \n",
        "        # if image has no grayscale color channel, add one\n",
        "        if(len(roi.shape) == 2):\n",
        "            # add that third color dim\n",
        "            roi = roi.reshape(roi.shape[0], roi.shape[1], 1)\n",
        "        \n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        roi = roi.transpose((2, 0, 1))\n",
        "        \n",
        "        roi = torch.from_numpy(roi)\n",
        "        roi = roi.type(torch.cuda.FloatTensor)\n",
        "        \n",
        "        roi.unsqueeze_(0)\n",
        "        ## Make facial keypoint predictions using your loaded, trained network     \n",
        "        ## perform a forward pass to get the predicted facial keypoints\n",
        "        \n",
        "        # forward pass to get net output\n",
        "        output_pts = net(roi)\n",
        "        # reshape to size x 68 x 2 pts\n",
        "        output_pts = output_pts.view(68, -1)\n",
        "        \n",
        "        # undo normalization of keypoints\n",
        "        output_pts = output_pts.detach().cpu().numpy()   \n",
        "        output_pts = output_pts*(roi_copy.shape[0]/4)+roi_copy.shape[0]/2\n",
        "        \n",
        "        ## Display each detected face and the corresponding keypoints     \n",
        "        fig.add_subplot(2, 2, i+1)\n",
        "        plt.imshow(roi_copy, cmap='gray')\n",
        "        plt.scatter(output_pts[:, 0], output_pts[:, 1], s=5, marker='.', c='m')\n",
        "        plt.axis('off')\n",
        "        i += 1\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "L5NrtnFfG16-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect_keypoints(0,30,resnet18)\n",
        "detect_keypoints(2,30,resnet18)\n",
        "detect_keypoints(1,40,resnet18)"
      ],
      "metadata": {
        "id": "RbtFMe41G4Gk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}